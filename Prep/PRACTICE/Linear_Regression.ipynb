{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0338ee2",
   "metadata": {},
   "source": [
    "##Problem Statement: Predicting the House prices in California using various socioeconomic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e99a3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451caa8a",
   "metadata": {},
   "source": [
    "#Here we are fetching the data from the california housing \n",
    "#If we won't use as_frame = True \n",
    "#Then we will get the Bunch object with \n",
    "#.data: a numpy array of feature, .target: a array of targets, .feature_names: list of feature names\n",
    "#If we use the as_frame = True\n",
    "#Then also we will get the Bunch object, but\n",
    "#.data: a pandas Dataframe\n",
    "#.target: a pandas series \n",
    "#.frame: Combined Dataframe of features + target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95fae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing(as_frame = True)\n",
    "data_woaf = fetch_california_housing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27946ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    df = data.frame\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b23da74",
   "metadata": {},
   "source": [
    "#Splitting the dependent and independet Features in X and y\n",
    "#Here we dropped the the target column \n",
    "#Where axis = 1 means we are drop the MedHouseVal in column manner \n",
    "#axis = 1 for row wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7905d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(\"MedHouseVal\", axis = 1)\n",
    "y = df[\"MedHouseVal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4112a",
   "metadata": {},
   "source": [
    "#Here we are normalizing the numerical features into mean =0 and Standard Deviation = 1 by using the Standard Scaler\n",
    "#fit is used to calculate the meand and Standard Deviation of each feature(cloumn) in X\n",
    "#transform is used to scale the data by using values we got from fit, formulae if X_scaled = (X - mean)/(StandardDeviation)\n",
    "#Finally, we got the X_scaled with the mean = 0 and Standard Deviation =1 for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc44bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201fe343",
   "metadata": {},
   "source": [
    "##We are splitting the data into train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ce99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4331c87",
   "metadata": {},
   "source": [
    "#We are training the Model with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd0125a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ccc31c",
   "metadata": {},
   "source": [
    "#Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f7007ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Absolute Error:  0.5332001304956563\n",
      "Root Mean Squared Error:  0.7455813830127763\n",
      " Mean Squared Error:  0.5558915986952442\n",
      "R2 score:  0.575787706032451\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score\n",
    "y_pred = model.predict(X_test)\n",
    "#average error in original units like inputs, all errors are equally important, that means small and large errors are equally considered\n",
    "print(\"Root Mean Absolute Error: \", mean_absolute_error(y_test, y_pred))\n",
    "#average error in original units and large errors are heavily penalized\n",
    "print(\"Root Mean Squared Error: \", mean_squared_error(y_test, y_pred, squared=False))\n",
    "#large errors are heavily penalized but not in the same units as the original data\n",
    "print(\" Mean Squared Error: \", mean_squared_error(y_test, y_pred))\n",
    "#It measures the variance, that means how model in prefroming on the test dataset\n",
    "print(\"R2 score: \", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d4429f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Alpha (Ridge): 10.0\n",
      "Root Mean Absolute Error:  0.5332432567214163\n",
      "Root Mean Squared Error:  0.7450104388391259\n",
      " Mean Squared Error:  0.5550405539792669\n",
      "R2 score:  0.5764371557310631\n",
      "Best Aplha (Lasso):  0.001\n",
      "Root Mean Absolute Error:  0.5332857888017251\n",
      "Root Mean Squared Error:  0.7442405630689863\n",
      " Mean Squared Error:  0.5538940157172418\n",
      "R2 score:  0.5773121026225017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "ridge_model = RidgeCV(alphas = [0.001, 0.01, 0.1, 1.0, 10.0])\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "print(\"Best Alpha (Ridge):\", ridge_model.alpha_)\n",
    "#average error in original units like inputs, all errors are equally important, that means small and large errors are equally considered\n",
    "print(\"Root Mean Absolute Error: \", mean_absolute_error(y_test, y_pred_ridge))\n",
    "#average error in original units and large errors are heavily penalized\n",
    "print(\"Root Mean Squared Error: \", mean_squared_error(y_test, y_pred_ridge, squared=False))\n",
    "#large errors are heavily penalized but not in the same units as the original data\n",
    "print(\" Mean Squared Error: \", mean_squared_error(y_test, y_pred_ridge))\n",
    "#It measures the variance, that means how model in prefroming on the test dataset\n",
    "print(\"R2 score: \", r2_score(y_test, y_pred_ridge))\n",
    "\n",
    "lasso_model = LassoCV(alphas = [0.001, 0.01, 0.1, 1.0, 10.0])\n",
    "lasso_model.fit(X_train, y_train)\n",
    "print(\"Best Aplha (Lasso): \", lasso_model.alpha_)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "#average error in original units like inputs, all errors are equally important, that means small and large errors are equally considered\n",
    "print(\"Root Mean Absolute Error: \", mean_absolute_error(y_test, y_pred_lasso))\n",
    "#average error in original units and large errors are heavily penalized\n",
    "print(\"Root Mean Squared Error: \", mean_squared_error(y_test, y_pred_lasso, squared=False))\n",
    "#large errors are heavily penalized but not in the same units as the original data\n",
    "print(\" Mean Squared Error: \", mean_squared_error(y_test, y_pred_lasso))\n",
    "#It measures the variance, that means how model in prefroming on the test dataset\n",
    "print(\"R2 score: \", r2_score(y_test, y_pred_lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c04adb0",
   "metadata": {},
   "source": [
    "##Adding the polynomial features\n",
    "#include_bias = false means donot add a column of 1s to my features\n",
    "#why a columns of 1's are added at first?\n",
    "if X = [[2], [3], [4]] then it changes to X_poly = [[1, 2, 4], [1, 3, 9], [1, 4, 16]]\n",
    "#1 is used by the model to learn the intercept(bias). The intercept is the value of y when x=0 it helps the model shift the predict lineup or down\n",
    "#Linear Regressor already adds that automatically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b388bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfa1ae1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Alpha (Ridge): 100.0\n",
      "Root Mean Absolute Error:  0.4765235086008149\n",
      "Root Mean Squared Error:  0.6690192335852048\n",
      " Mean Squared Error:  0.4475867349069348\n",
      "R2 score:  0.6584373715847264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree = 2, include_bias = False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size = 0.2, random_state = 42)\n",
    "ridge_poly_model = RidgeCV(alphas = [0.01, 0.1, 1, 10, 20, 30, 50, 70, 100])\n",
    "ridge_poly_model.fit(X_train, y_train)\n",
    "y_pred_ridge_poly = ridge_poly_model.predict(X_test)\n",
    "print(\"Best Alpha (Ridge):\", ridge_poly_model.alpha_)\n",
    "#average error in original units like inputs, all errors are equally important, that means small and large errors are equally considered\n",
    "print(\"Root Mean Absolute Error: \", mean_absolute_error(y_test, y_pred_ridge_poly))\n",
    "#average error in original units and large errors are heavily penalized\n",
    "print(\"Root Mean Squared Error: \", mean_squared_error(y_test, y_pred_ridge_poly, squared=False))\n",
    "#large errors are heavily penalized but not in the same units as the original data\n",
    "print(\" Mean Squared Error: \", mean_squared_error(y_test, y_pred_ridge_poly))\n",
    "#It measures the variance, that means how model in prefroming on the test dataset\n",
    "print(\"R2 score: \", r2_score(y_test, y_pred_ridge_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b85626ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score:  0.4825038620323169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 262.2401812361604, tolerance: 1.7549703365115017\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2654.4109401013297, tolerance: 1.7549703365115017\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.59024320562003, tolerance: 1.778593639468773\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2062.671718513492, tolerance: 1.778593639468773\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 179.18122748633414, tolerance: 1.762635966621353\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1666.5557884945692, tolerance: 1.762635966621353\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2659.351976320445, tolerance: 1.7761062572595387\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 181.31791679751586, tolerance: 1.7567356598263502\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1896.357282454545, tolerance: 1.7567356598263502\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    }
   ],
   "source": [
    "lasso_poly_model = LassoCV(alphas = [0.001,0.01, 0.1, 1, 10, 20, 30, 50, 70, 100], max_iter = 1000)\n",
    "lasso_poly_model.fit(X_train, y_train)\n",
    "lasso_poly_predict = lasso_poly_model.predict(X_test)\n",
    "print(\"R2 score: \", r2_score(y_test, lasso_poly_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52604bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\subprocess.py\", line 493, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\subprocess.py\", line 858, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\jaswe\\OneDrive\\Desktop\\opt\\Projects\\mlproject\\venv\\lib\\subprocess.py\", line 1327, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param (KNN):  {'n_neighbors': 10, 'weights': 'distance'}\n",
      "Best R2 Score:  0.697594984371358\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': list(range(1, 31)),\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "knn_model = KNeighborsRegressor()\n",
    "grid_knn = GridSearchCV(knn_model, param_grid_knn, cv =5, scoring = 'r2')\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "print('Best Param (KNN): ', grid_knn.best_params_)\n",
    "print('Best R2 Score: ', grid_knn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3af56b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Best R2 score:  0.7001265228313104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "param_grid_tree = {\n",
    "    'max_depth':[None, 5, 10, 20],\n",
    "    #only these many samples are there, then only split\n",
    "    'min_samples_split':[2, 5, 10],\n",
    "    #leaf node have atleast these many samples\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "tree = DecisionTreeRegressor(random_state =42)\n",
    "grid_tree = GridSearchCV(tree, param_grid_tree, cv =5, scoring = 'r2')\n",
    "grid_tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params: \", grid_tree.best_params_)\n",
    "print(\"Best R2 score: \", grid_tree.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7863171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Absolute Error:  0.43377592257949354\n",
      "Root Mean Squared Error:  0.6432874980544813\n",
      " Mean Squared Error:  0.4138188051531942\n",
      "R2 score:  0.684206372190225\n",
      "Root Mean Absolute Error:  0.4201477174764044\n",
      "Root Mean Squared Error:  0.6316246180077832\n",
      " Mean Squared Error:  0.3989496580734781\n",
      "R2 score:  0.695553323658519\n"
     ]
    }
   ],
   "source": [
    "best_knn = grid_knn.best_estimator_\n",
    "best_tree = grid_tree.best_estimator_\n",
    "y_pred_knn = best_knn.predict(X_test)\n",
    "y_pred_tree = best_tree.predict(X_test)\n",
    "#average error in original units like inputs, all errors are equally important, that means small and large errors are equally considered\n",
    "print(\"Root Mean Absolute Error: \", mean_absolute_error(y_test, y_pred_knn))\n",
    "#average error in original units and large errors are heavily penalized\n",
    "print(\"Root Mean Squared Error: \", mean_squared_error(y_test, y_pred_knn, squared=False))\n",
    "#large errors are heavily penalized but not in the same units as the original data\n",
    "print(\" Mean Squared Error: \", mean_squared_error(y_test, y_pred_knn))\n",
    "#It measures the variance, that means how model in prefroming on the test dataset\n",
    "print(\"R2 score: \", r2_score(y_test, y_pred_knn))\n",
    "#average error in original units like inputs, all errors are equally important, that means small and large errors are equally considered\n",
    "print(\"Root Mean Absolute Error: \", mean_absolute_error(y_test, y_pred_tree))\n",
    "#average error in original units and large errors are heavily penalized\n",
    "print(\"Root Mean Squared Error: \", mean_squared_error(y_test, y_pred_tree, squared=False))\n",
    "#large errors are heavily penalized but not in the same units as the original data\n",
    "print(\" Mean Squared Error: \", mean_squared_error(y_test, y_pred_tree))\n",
    "#It measures the variance, that means how model in prefroming on the test dataset\n",
    "print(\"R2 score: \", r2_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12efd22f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
